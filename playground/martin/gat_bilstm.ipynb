{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from seiz_eeg.dataset import EEGDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from training import train\n",
    "from utils import display_metrics, count_parameters, seed_everything\n",
    "from preprocessing import normalize_z_score\n",
    "\n",
    "import signatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a29d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to change this according to where you store the data folder\n",
    "# Inside your data folder, you should have the following structure:\n",
    "# data\n",
    "# ├── train\n",
    "# │   ├── signals/\n",
    "# │   ├── segments.parquet\n",
    "# │-- test\n",
    "#     ├── signals/\n",
    "#     ├── segments.parquet\n",
    "\n",
    "data_path = \"data\"\n",
    "\n",
    "DATA_ROOT = Path(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clips_tr = pd.read_parquet(DATA_ROOT / \"train/segments.parquet\")\n",
    "clips_te = pd.read_parquet(DATA_ROOT / \"test/segments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59681537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on https://www.sciencedirect.com/science/article/pii/S1746809422004098#fig4\n",
    "\"\"\"\n",
    "\n",
    "class SlidingWindowBatch(nn.Module):\n",
    "    #TODO assert last window shape\n",
    "    def __init__(self, window_size=125, step_size=62):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, signal_len, channels = data.shape\n",
    "        num_windows = (signal_len - self.window_size) // self.step_size + 1\n",
    "\n",
    "        windows = []\n",
    "        for i in range(num_windows):\n",
    "            start = i * self.step_size\n",
    "            end = start + self.window_size\n",
    "            window = data[:, start:end, :]  # shape: (batch_size, window_size, 19)\n",
    "            windows.append(window)\n",
    "\n",
    "        windows = torch.stack(windows, dim=1)  # shape: (batch_size, num_windows, window_size, 19)\n",
    "        return windows.transpose(-2,-1) # shape: (batch_size, num_windows, 19, window_size)\n",
    "    \n",
    "class SignatureEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, depth=2, add_time=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_channels (int): number of input channels (without time).\n",
    "            depth (int): signature depth.\n",
    "            add_time (bool): whether to augment with time channel.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.depth = depth\n",
    "        self.add_time = add_time\n",
    "        \n",
    "        self.total_channels = input_channels + 1 if add_time else input_channels\n",
    "        self.output_channels = signatory.signature_channels(self.total_channels, self.depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): shape (batch_size, windows, channels, time)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: shape (batch_size, windows, output_channels)\n",
    "        \"\"\"\n",
    "        batch_size, n_windows, n_time, n_channels = x.shape\n",
    "\n",
    "        # Step 2: flatten batch and window dimensions\n",
    "        x = x.reshape(batch_size * n_windows, n_time, n_channels)\n",
    "\n",
    "        # Step 3: add time if needed\n",
    "        if self.add_time:\n",
    "            time = torch.linspace(0, 1, n_time, device=x.device).unsqueeze(0).unsqueeze(-1)\n",
    "            time = time.expand(x.size(0), -1, -1)  # (batch_size * windows, time, 1)\n",
    "            x = torch.cat([time, x], dim=-1)  # (batch_size * windows, time, channels+1)\n",
    "\n",
    "        # Step 4: compute signature\n",
    "        sig = signatory.signature(x, depth=self.depth)  # (batch_size * windows, output_channels)\n",
    "\n",
    "        # Step 5: reshape back to (batch_size, windows, output_channels)\n",
    "        sig = sig.view(batch_size, n_windows, self.output_channels)\n",
    "\n",
    "        return sig\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gcn1 = GCNConv(in_channels=in_channels, out_channels=hidden_channels)\n",
    "\n",
    "        self.gcn2 = GCNConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # First GAT layer\n",
    "        x = self.gcn1(x, edge_index)  # shape: (batch_size * num_windows, 19, hidden_channels * num_heads_1)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Second GAT layer with 1 attention head\n",
    "        x = self.gcn2(x, edge_index)  # shape: (batch_size * num_windows, 19, hidden_channels)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, attn_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: dimension of BiLSTM output (hidden_size * 2 if BiLSTM)\n",
    "            attn_dim: dimension of intermediate MLP hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention_mlp = nn.Linear(hidden_dim, attn_dim)\n",
    "        self.context_vector = nn.Parameter(torch.randn(attn_dim))\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: Tensor of shape (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            s: Tensor of shape (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Step 1: compute ut = tanh(Wh + b)\n",
    "        u = torch.tanh(self.attention_mlp(h))  # (batch_size, seq_len, attn_dim)\n",
    "\n",
    "        # Step 2: compute scores: dot(u_t, u_w)\n",
    "        # context_vector: (attn_dim,)\n",
    "        scores = torch.matmul(u, self.context_vector)  # (batch_size, seq_len)\n",
    "\n",
    "        # Step 3: softmax over time\n",
    "        alpha = F.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Step 4: weighted sum of h\n",
    "        s = torch.bmm(alpha.unsqueeze(1), h)  # (batch_size, 1, hidden_dim)\n",
    "        s = s.squeeze(1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        return s\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, gcn_in_features=64, gcn_out_features=32, output_size=1, window_size=125, step_size=62, lstm_input_size=128, lstm_hidden_size=128, attention_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sliding window batch layer\n",
    "        self.sliding_window = SlidingWindowBatch(window_size=window_size, step_size=step_size)\n",
    "\n",
    "        # Projection layer from 125 to 64 features\n",
    "        self.projection = nn.Linear(window_size, gcn_in_features)\n",
    "\n",
    "        # GAT layers\n",
    "        self.gcn = GCNLayer(in_channels=gcn_in_features, hidden_channels=gcn_out_features)\n",
    "\n",
    "        # Flattening the output and passing through a fully connected layer before BiLSTM\n",
    "        self.fc = nn.Linear(19 * gcn_out_features, lstm_input_size)  # 19 nodes * 32 features per node\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size=lstm_input_size, hidden_size=lstm_hidden_size, num_layers=3, batch_first=True, bidirectional=True, dropout=0.5)\n",
    "\n",
    "        # Attention\n",
    "        self.attention = TemporalAttention(lstm_hidden_size * 2, attention_dim)\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(lstm_hidden_size * 2, output_size)  # *2 because of bidirectional\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply sliding window on input\n",
    "        windows = self.sliding_window(x)  # shape: (batch_size, num_windows, 19, window_size)\n",
    "\n",
    "        # Projection layer\n",
    "        windows_proj = self.projection(windows)  # shape: (batch_size, num_windows, 19, 64)\n",
    "\n",
    "        # Reshape to (batch_size * num_windows, 19, 64) for GCN\n",
    "        batch_size, num_windows, nodes, features = windows_proj.shape\n",
    "        windows_proj = windows_proj.view(batch_size * num_windows, nodes, features)  # shape: (batch_size * num_windows, 19, 64)\n",
    "        \n",
    "        # Make the tensor contiguous before passing it to GCN\n",
    "        windows_proj = windows_proj.contiguous()\n",
    "\n",
    "        # Apply GCN\n",
    "        gcn_out = self.gcn(windows_proj, edge_index)  # shape: (batch_size * num_windows, 19, 32)\n",
    "\n",
    "        # Reshape back to (batch_size, num_windows, 19, 32)\n",
    "        gcn_out = gcn_out.view(batch_size, num_windows, nodes, -1)  # shape: (batch_size, num_windows, 19, 32)\n",
    "\n",
    "        # Flatten each window and pass through fully connected layer\n",
    "        windows_flat = gcn_out.view(batch_size, num_windows, -1)  # shape: (batch_size, num_windows, 19 * 32)\n",
    "        \n",
    "        # Make the tensor contiguous before passing to FC layer\n",
    "        windows_flat = windows_flat.contiguous()\n",
    "        windows_flat = F.gelu(self.fc(windows_flat))  # shape: (batch_size, num_windows, 128)\n",
    "\n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.bilstm(windows_flat)  # shape: (batch_size, num_windows, lstm_hidden_size * 2)\n",
    "\n",
    "        # Apply Attention\n",
    "        weighted_attention = self.attention(lstm_out) # shape: (batch_size, lstm_hidden_size * 2)\n",
    "        \n",
    "        # Final output layer\n",
    "        out = self.output_layer(weighted_attention)  # shape: (batch_size, output_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "MAX_DIST = 1\n",
    "NUM_EPOCHS = 13\n",
    "model = CombinedModel(gat_in_features=64, gat_out_features=32, output_size=1, window_size=125, step_size=62, num_heads_1=8, num_heads_2=1, lstm_hidden_size=128)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71469d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the signal_transform, or remove it completely\n",
    "dataset_tr = EEGDataset(\n",
    "    clips_tr,\n",
    "    signals_root=DATA_ROOT / \"train\",\n",
    "    #signal_transform=normalize_z_score,\n",
    "    prefetch=True,  # If your compute does not allow it, you can use `prefetch=False`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "\n",
    "distance_matrix = torch.tensor(pd.read_csv('data/distances_3d.csv').pivot(index='from', columns='to', values='distance').to_numpy(),device=device,dtype=torch.float32)\n",
    "adjacency = (distance_matrix <= MAX_DIST).int()# - torch.eye(19).to(device)\n",
    "edge_index = torch.argwhere(adjacency==1).transpose(-1,-2).to(torch.long)\n",
    "\n",
    "#train_set, test_set, val_set = torch.utils.data.random_split(dataset_tr,[0.7,0.2,0.1])\n",
    "train_set, val_set = torch.utils.data.random_split(dataset_tr,[0.9,0.1])\n",
    "\n",
    "#Check worker_init_fn\n",
    "loader_tr = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=0)\n",
    "loader_val = DataLoader(val_set, batch_size=512, shuffle=True, num_workers=0)\n",
    "#loader_ts = DataLoader(test_set, batch_size=512, shuffle=True, num_workers=0)\n",
    "\n",
    "positives = 0\n",
    "negatives = 0\n",
    "for idx, data in enumerate(loader_tr):\n",
    "    positives += data[1].sum()\n",
    "    negatives += len(data[1]) - data[1].sum()\n",
    "\n",
    "pos_weight = negatives / positives\n",
    "pos_weight_tensor = torch.tensor([pos_weight]).to(torch.float32).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059690be",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train(model, NUM_EPOCHS, device, loader_tr, loader_val, optimizer, criterion, verbose=True, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(NUM_EPOCHS, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221e36c",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3540750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "dataset_te = EEGDataset(\n",
    "    clips_te,  # Your test clips variable\n",
    "    signals_root=DATA_ROOT\n",
    "    / \"test\",  # Update this path if your test signals are stored elsewhere\n",
    "    #signal_transform=fft_filtering,  # You can change or remove the signal_transform as needed\n",
    "    prefetch=True,  # Set to False if prefetching causes memory issues on your compute environment\n",
    "    return_id=True,  # Return the id of each sample instead of the label\n",
    ")\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "loader_te = DataLoader(dataset_te, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21088b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the submission file for Kaggle\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store sample IDs and predictions\n",
    "all_predictions = []\n",
    "all_ids = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for x, ids in loader_te:\n",
    "        # Assume each batch returns a tuple (x_batch, sample_id)\n",
    "        # If your dataset does not provide IDs, you can generate them based on the batch index.\n",
    "\n",
    "        # Move the input data to the device (GPU or CPU)\n",
    "        x = x.to(torch.float32).to(device)\n",
    "\n",
    "        # Perform the forward pass to get the model's output logits\n",
    "        logits = model(x, edge_index)\n",
    "\n",
    "        # Convert logits to predictions.\n",
    "        # For binary classification, threshold logits at 0 (adjust this if you use softmax or multi-class).\n",
    "        predictions = torch.round(torch.sigmoid(logits)).cpu().numpy()\n",
    "\n",
    "        # Append predictions and corresponding IDs to the lists\n",
    "        all_predictions.extend(predictions.flatten().tolist())\n",
    "        all_ids.extend(list(ids))\n",
    "\n",
    "# Create a DataFrame for Kaggle submission with the required format: \"id,label\"\n",
    "submission_df = pd.DataFrame({\"id\": all_ids, \"label\": all_predictions})\n",
    "submission_df[\"id\"] = submission_df[\"id\"].apply(lambda x: \"_\".join([txt.replace(\"_\",\"\") for txt in x.split(\"__\")]))\n",
    "submission_df[\"label\"] = submission_df[\"label\"].astype(int)\n",
    "\n",
    "submission_df.to_csv(\"submission_seed1.csv\", index=False)\n",
    "print(\"Kaggle submission file generated: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
