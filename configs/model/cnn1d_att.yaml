module:
  _target_: helpers.models.cnn1d_att.CNN1DAttentionClassifier
  series_length: 3000  # Length of the input time series
  num_nodes: 19  # Number of nodes (channels)
  embed_dim: 128  # Embedding dimension for tokenization
  num_cnn_layers: 2  # Number of CNN layers for token embedding
  num_transformer_layers: 2  # Number of transformer layers
  num_heads: 4  # Number of attention heads in the transformer
  mlp_dim: 64  # Feedforward layer dimension in the transformer
  dropout: 0.1  # Dropout rate
signal_transform: time_filtering
normalize: False