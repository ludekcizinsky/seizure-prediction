# Configuration for the baseline LSTM model provided by the TAs
# ------------------------------------------------------------------------------------------
username: 'cizinsky' # your izar username

seed: 42
debug: True # to avoid logging into wandb
output_dir: /scratch/izar/${username}/netml/outputs


# w&b parameters
logger:
  project: seizure-prediction
  tags: [dev, baseline] # something we can filter by

data:
  batch_size: 512
  num_workers: 16
  root: /scratch/izar/${username}/netml/data
  prefetch: True # If your compute does not allow it, you can use `prefetch=False`
  signal_transform: fft_filtering

model:
  temb_dim: 256 
  weight_dim: 96 # assuming mixed tokenization
  num_weight_tokens: 61 # assuming mixed tokenization
  hidden_dim: 256 
  num_layers: 8
  num_heads: 4
  lr: 2e-4
  weight_decay: 0.0
  warmup_epochs: 10
  warmup_factor: 0.0


trainer:
  max_epochs: 1000
  accelerator: gpu
  devices: 1
  checkpoint_every_n_epochs: 250
  grad_clip: 0.0
  precision: 32


# Just to let Hydra know where to save the config file
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: False