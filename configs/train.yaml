# Most basic config for training a model

# Set the model to use, specify in model directory
defaults:
  - model: baseline
  - _self_

# General info
username: 'mlebras' # your izar username
seed: 42
debug: False # to avoid logging into wandb, instead we log locally using Tensorboard
output_dir: /scratch/izar/${username}/netml/outputs # will be created if it doesn't exist

# w&b 
logger:
  project: seizure-prediction # name of the project in wandb, accessible to public
  tags: [baseline] # practical when trying to filter experiments

# Data 
data:
  subset: -1 # -1 = disable, else int, we have around 11k samples
  trn_frac: 0.9
  batch_size: 512
  num_workers: 8
  root: /scratch/izar/${username}/netml/data # path to the data
  prefetch: True # should be always true, unless you are on your laptop

# Optimisation
optim:
  lr: 5e-4
  weight_decay: 0.0
  warmup_epochs: 50

  # Scheduler - reduce on plateau
  plateau_patience: 10 # how many val/f1 to wait before reducing
  plateau_factor: 0.5 # reduce by this factor
  min_lr: 1e-6  # minimum learning rate

  # Gradient clipping
  max_grad_norm: 1.0 # gradient clipping
  grad_norm_type: 2.0 # norm type for gradient clipping

# PL trainer 
trainer:
  max_epochs: 1000
  accelerator: gpu
  devices: 1
  checkpoint_every_n_epochs: 250
  check_val_every_n_epoch: 25
  precision: 32


# Just to let Hydra know where to save the config file
hydra:
  run:
    dir: ${output_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: False