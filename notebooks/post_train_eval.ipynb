{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"~/netml-project\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from helpers.pl_module import SeizurePredictor\n",
    "from helpers.dataset import get_dataloaders, get_datasets\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from torchmetrics.functional.classification import f1_score, binary_accuracy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First things first, make sure that below you have your username if on izar, else, make sure that the CKPT_DIR is set to the correct path - where you want to download the checkpoint. In additin, also make sure that the SUBMISSION_DIR is set to the correct path - where you want to save the submission file. Ideally, you should save it in the root of the netml-project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"cizinsky\"\n",
    "CKPT_DIR = f\"/scratch/izar/{username}/netml/outputs/tmp\"\n",
    "SUBMISSION_DIR = f\"/home/{username}/netml-project/submissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $CKPT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the best model from wandb and load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the runs by tag for instance ang get an overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shza7rfa | northern-monkey-67 | tags: ['baseline', 'fft', 'lstm', 'part1'] | val/f1: 0.7631089091300964\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"ludekcizinsky/seizure-prediction\")\n",
    "\n",
    "tagged_runs = [run for run in runs if \"baseline\" in run.tags]\n",
    "\n",
    "for run in tagged_runs:\n",
    "    print(f\"{run.id} | {run.name} | tags: {run.tags} | val/f1: {run.summary.get('val/f1')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can choose specific run (based on the run id - most left column) and download the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to /scratch/izar/cizinsky/netml/outputs/tmp.\n"
     ]
    }
   ],
   "source": [
    "run_id = \"shza7rfa\"\n",
    "run = next((run for run in runs if run.id == run_id), None)\n",
    "assert run is not None, \"Run not found!\"\n",
    "\n",
    "artifact_path = f\"ludekcizinsky/seizure-prediction/model-{run_id}:best\"\n",
    "artifact = api.artifact(artifact_path, type=\"model\")\n",
    "artifact.download(CKPT_DIR)\n",
    "print(f\"Downloaded checkpoint to {CKPT_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the model from the checkpoint and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_module = SeizurePredictor.load_from_checkpoint(f\"{CKPT_DIR}/model.ckpt\")\n",
    "pl_module.eval().freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Finally, inference time! Let's start with loading the val data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FYI: using the following signal transform: fft_filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cizinsky/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/cizinsky/venvs/netml/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/cizinsky/venvs/netml/lib/python3.10/site-pack ...\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "_, val_dataloader = get_dataloaders(pl_module.hparams)\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, logger=False, callbacks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can predict the outputs, and map them into single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01207f684ae4a02883c37884c252830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict\n",
    "outputs = trainer.predict(pl_module, val_dataloader)\n",
    "\n",
    "# Map into single tensor\n",
    "preds = torch.cat([output[\"preds_batch\"] for output in outputs])\n",
    "y = torch.cat([output[\"y_batch\"] for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9300, F1: 0.8800\n"
     ]
    }
   ],
   "source": [
    "acc = binary_accuracy(preds, y)\n",
    "neg_f1, pos_f1 = f1_score(preds, y, task=\"multiclass\", num_classes=2, average=None)\n",
    "f1_macro = torch.mean(torch.stack([neg_f1, pos_f1]))\n",
    "print(f\"Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}, F1 (neg): {neg_f1:.4f}, F1 (pos): {pos_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have loaded the model, evaluated it on the val set, we can test it on the test set. Let's start loading the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_datasets(pl_module.hparams, split=\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=pl_module.hparams.data.batch_size,\n",
    "    num_workers=pl_module.hparams.data.num_workers,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the inference and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(pl_module, test_dataloader)\n",
    "preds = torch.cat([output[\"preds_batch\"] for output in outputs]).cpu().numpy()\n",
    "sample_ids = []\n",
    "for output in outputs:\n",
    "    sample_ids.extend(output[\"y_batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"id\": sample_ids, \"label\": preds})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_path = os.path.join(SUBMISSION_DIR, f\"run_{run_id}.csv\")\n",
    "submission_df.to_csv(subm_path, index=False)\n",
    "print(f\"Kaggle submission file generated: {subm_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
